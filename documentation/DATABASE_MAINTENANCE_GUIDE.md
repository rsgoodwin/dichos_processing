# DATABASE MAINTENANCE GUIDE
## Adding New Dichos and Maintaining Database Integrity

This guide provides a step-by-step process for maintaining and updating the `dichos_normalized.db` database when new Costa Rican dichos are discovered. The process ensures data quality, maintains semantic clustering, and keeps the database clean and efficient.

---

## üìã **PREREQUISITES**

### **Required Environment:**
- Python 3.12+ with virtual environment
- SQLite3 database: `dichos_normalized.db`
- Access to LLM services (OpenAI, Anthropic, etc.) for text analysis
- Local NLP capabilities for semantic clustering

### **Current Database State:**
- **301 dichos** across **13 semantic clusters**
- **Multi-cluster assignments** (up to 3 per dicho)
- **Enhanced clusters** with descriptions and icons
- **Clean schema** with no redundant tables

---

## üîÑ **MAINTENANCE WORKFLOW OVERVIEW**

```
WhatsApp Dump ‚Üí Parse & Extract ‚Üí Filter New ‚Üí LLM Analysis ‚Üí Enrich Data ‚Üí NLP Clustering ‚Üí Update Database ‚Üí Clean & Optimize
```

---

## üì• **STEP 1: READ AND PARSE WHATSAPP TEXT DUMP**

### **Objective:** Extract datetime, contributor, and text from new WhatsApp chat export

### **Process:**
1. **Obtain new WhatsApp export** (text format)
2. **Parse each line** to extract:
   - `date_time`: Timestamp of message
   - `contributor_first`: First name of sender
   - `contributor_last`: Last name of sender (if available)
   - `text`: Full message content

### **Expected Format:**
```
8/17/25 4:58 PM - Javier Soto: No se haga la barba en seco.
8/17/25 5:02 PM - Maria Rodriguez: That's a good one!
8/17/25 5:15 PM - Carlos Mendez: El que mucho abarca, poco aprieta.
```

### **Output:** CSV or structured data with parsed fields

---

## üÜï **STEP 2: IDENTIFY NEW DICHOS**

### **Objective:** Determine which rows contain new dichos based on timestamp comparison

### **Process:**
1. **Query current database** for most recent `date_time`:
   ```sql
   SELECT MAX(date_time) FROM dichos;
   ```

2. **Filter new rows** where `date_time > most_recent_in_db`

3. **Remove duplicates** by comparing text content with existing dichos

4. **Identify potential dichos** vs. commentary (preliminary filtering)

### **Output:** List of new, unique messages that need processing

---

## ü§ñ **STEP 3: LLM ANALYSIS - DICHO IDENTIFICATION**

### **Objective:** Use LLM to determine which new rows contain actual Spanish dichos vs. commentary

### **Process:**
1. **Send new messages to LLM** with prompt:
   ```
   Analyze this WhatsApp message and determine if it contains a Costa Rican dicho (proverb/saying):
   
   Message: [MESSAGE_TEXT]
   Sender: [CONTRIBUTOR]
   Timestamp: [DATETIME]
   
   Respond with:
   - IS_DICHO: true/false
   - REASON: Brief explanation
   - SPANISH_TEXT: Extract the dicho if present
   - CONFIDENCE: 0-1 score
   ```

2. **Filter results** to keep only rows where `IS_DICHO = true` and `CONFIDENCE > 0.7`

3. **Extract Spanish text** for confirmed dichos

### **Output:** Filtered list of confirmed new dichos with extracted Spanish text

---

## üéØ **STEP 4: LLM ENRICHMENT - COMPLETE METADATA**

### **Objective:** Use LLM to enrich each new dicho with complete metadata matching existing schema

### **Process:**
1. **For each confirmed dicho**, send to LLM with enrichment prompt:
   ```
   Enrich this Costa Rican dicho with complete metadata:
   
   Dicho: [SPANISH_TEXT]
   
   Provide:
   - translation: English translation
   - expanded_context_usage: Detailed explanation of when/how to use
   - semantic_keywords: 5-10 English keywords capturing meaning
   - cultural_context: Cultural background (Tico-specific or universal)
   - emotion_tone: Emotional tone (advice, warning, humor, etc.)
   - difficulty_level: 1-5 scale for learners
   - learning_notes: Language learning insights
   ```

2. **Validate LLM output** for consistency and quality
3. **Standardize semantic_keywords** (lowercase, underscore_separated, no duplicates)

### **Output:** Enriched dichos with complete metadata matching existing schema

---

## üîç **STEP 5: KEYWORD ANALYSIS AND CLUSTER COMPATIBILITY**

### **Objective:** Analyze new semantic keywords and ensure they fit within existing cluster structure

### **Process:**
1. **Extract all new semantic_keywords** from new dichos
2. **Compare with existing keywords** in clusters table:
   ```sql
   SELECT DISTINCT keywords FROM clusters;
   ```
3. **Identify new keywords** that don't exist in current clusters
4. **Analyze semantic compatibility** with existing clusters
5. **Plan cluster expansion** if new keywords require new clusters

### **Output:** Assessment of whether new dichos fit existing clusters or require schema changes

---

## üß† **STEP 6: NLP RECLUSTERING (LOCAL PROCESSING)**

### **Objective:** Use local NLP capabilities to assign new dichos to existing clusters

### **Process:**
1. **Load existing clustering model** (SentenceTransformer 'all-MiniLM-L6-v2')
2. **Generate embeddings** for:
   - All existing cluster keywords
   - New dicho semantic_keywords
3. **Calculate similarity scores** between new dichos and existing clusters
4. **Assign clusters** using same logic as current system:
   - Primary cluster (best match)
   - Secondary clusters (within 10% gap or above 0.35 threshold)
   - Tertiary clusters (above 0.3 threshold)
   - Maximum 3 clusters per dicho

### **Output:** Cluster assignments for all new dichos

---

## üóÑÔ∏è **STEP 7: DATABASE UPDATES**

### **Objective:** Insert new dichos and update all related tables

### **Process:**
1. **Insert new dichos** into `dichos` table
2. **Insert cluster assignments** into `multi_cluster_assignments` table
3. **Update cluster sizes** in `clusters` table
4. **Verify foreign key constraints** and relationships
5. **Update any derived data** (e.g., total counts, statistics)

### **Output:** Updated database with new dichos and cluster assignments

---

## üßπ **STEP 8: DATABASE CLEANUP AND OPTIMIZATION**

### **Objective:** Maintain database performance and cleanliness

### **Process:**
1. **Reindex database** for optimal performance:
   ```sql
   REINDEX;
   ```

2. **Update statistics** for query optimizer:
   ```sql
   ANALYZE;
   ```

3. **Verify data integrity**:
   - Check foreign key relationships
   - Validate cluster assignments
   - Confirm no orphaned records

4. **Clean up temporary files** and intermediate data

### **Output:** Clean, optimized database ready for application use

---

## üìä **STEP 9: VALIDATION AND TESTING**

### **Objective:** Ensure new data maintains quality and system integrity

### **Process:**
1. **Verify cluster distributions** are reasonable
2. **Test semantic similarity** calculations
3. **Validate multi-cluster assignments** follow established rules
4. **Check application queries** still work correctly
5. **Update documentation** with new cluster sizes and statistics

### **Output:** Validated system ready for production use

---

## üö® **IMPORTANT CONSIDERATIONS**

### **Data Quality:**
- **LLM outputs** should be validated for consistency
- **Semantic keywords** must follow established format
- **Cluster assignments** should maintain semantic coherence

### **Performance:**
- **Batch processing** for large numbers of new dichos
- **Incremental updates** to avoid full reclustering
- **Regular maintenance** to prevent database bloat

### **Scalability:**
- **Monitor cluster sizes** as they grow
- **Consider cluster splitting** if any become too large
- **Plan for keyword expansion** in existing clusters

---

## üîß **TECHNICAL IMPLEMENTATION NOTES**

### **Required Python Packages:**
```bash
pip install sentence-transformers pandas sqlite3 numpy plotly
```

### **Database Schema Changes:**
- **No schema changes** required for adding new dichos
- **Cluster sizes** will update automatically
- **Multi-cluster assignments** will expand as needed

### **File Management:**
- **Backup database** before major updates
- **Version control** for enrichment scripts
- **Log all changes** for audit purposes

---

## üìà **MONITORING AND MAINTENANCE**

### **Regular Tasks:**
- **Weekly**: Check for new WhatsApp exports
- **Monthly**: Review cluster distributions and quality
- **Quarterly**: Full database optimization and cleanup

### **Metrics to Track:**
- **Total dichos** and growth rate
- **Cluster sizes** and distribution
- **Semantic keyword coverage**
- **Database performance** and query times

---

## üÜò **TROUBLESHOOTING**

### **Common Issues:**
1. **LLM API limits** - Implement rate limiting and retry logic
2. **Cluster assignment failures** - Check similarity score thresholds
3. **Database performance** - Regular reindexing and cleanup
4. **Data consistency** - Validate LLM outputs before insertion

### **Recovery Procedures:**
- **Database rollback** from backups if needed
- **Partial reinsertion** of failed records
- **Manual cluster assignment** for edge cases

---

## üìö **RESOURCES AND REFERENCES**

### **Current System Documentation:**
- `ENHANCED_CLUSTERS_SUMMARY.md` - Cluster descriptions and icons
- `optimized_multi_cluster_queries.sql` - Query examples
- `DATABASE_CLEANUP_SUMMARY.md` - Schema optimization details

### **NLP Model Information:**
- **Model**: `all-MiniLM-L6-v2`
- **Embedding dimensions**: 384
- **Similarity calculation**: Cosine similarity
- **Clustering method**: K-means with silhouette analysis

---

## ‚úÖ **SUCCESS CRITERIA**

### **Maintenance Complete When:**
- [ ] All new dichos processed and enriched
- [ ] Cluster assignments completed
- [ ] Database updated and optimized
- [ ] Data integrity verified
- [ ] Application functionality tested
- [ ] Documentation updated

---

*This guide should be used in conjunction with the current database schema and clustering system. Always backup the database before making changes and test thoroughly in a development environment first.*
